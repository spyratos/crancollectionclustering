{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Programming - Assignment 2\n",
    "## Exploring the MNIST data set, PCA and agglomerative hierarchical clustering methods\n",
    "### Handed out: 10 November 2017 - Due by: 1 December 2017\n",
    "\n",
    "In this assignment you will experiment and evaluate the hierarchical agglomerative clustering methods under different settings. You will work on the MNIST data set, which is a collection of hand-written digits. Due to its manageable size and popularity MNIST is a dataset you can go to for evaluating new ideas, for replicating research results, etc.\n",
    "\n",
    "This assignment will comprise a series of small exercises. Some of the background material, e.g. `numpy`, `sklearn`, etc., will be presented during the next lectures, however you are expected to actively look for information on the web, using Python's `help` function, etc. \n",
    "\n",
    "The marks awarded for each exercise are indicated below and they total 35. The final mark will be rescaled to a maximum of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insert all imports in the cell immediately below and nowhere else in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.cluster import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch the MNIST data set if not already present in the current working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the MNIST dataset\n",
    "MNIST is one of the most famous ML datasets and it contains 70,000 hand-written digits and their labels. Using `sklearn` to fetch the dataset, as above, `mnist.data` contains the data, while `mnist.target` contains the labels. \n",
    "\n",
    "#### (1) Find how many data points there are available and what is their size [1 mark]\n",
    "#### (2) How many labels are there? [1 mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n",
      "['COL_NAMES', 'DESCR', 'data', 'target']\n"
     ]
    }
   ],
   "source": [
    "print mnist.data.shape\n",
    "print mnist.target.shape\n",
    "print dir(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Extract a random subset of the original data and labels and store them in variables `small_data` and `small_target`. In the remainder of this exercise we will work with these smaller sets. The cardinality of the `small_*` subsets should be 2000. [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import random as r\n",
    "r.seed(5)\n",
    "#print r.sample([0:69999],2000)\n",
    "something = r.sample( list(range(0, 69999)), 2000)\n",
    "small_data = mnist.data[something]\n",
    "small_target = mnist.target[something]\n",
    "show_digit(1500,small_data)\n",
    "print small_target[1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Create a function to display a single digit from the MNIST data set and its label. When done, use your function to display one of the digits. Use `small_data` (as opposed to `mnist.data`). [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_digit(ind, dataset=small_data):\n",
    "    \"\"\"Display the MNIST digit at index `ind` in grayscale\"\"\"\n",
    "    from PIL import Image\n",
    "    temp=dataset[ind].reshape(28,28)\n",
    "    img = Image.fromarray(temp, 'L')\n",
    "    img.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a pre-processing step either for further processing or visualisation we decompose raw data into their principal components using Principal Component Analysis (PCA). This allows us to reduce the dimensionality of the data while retaining as much of their original variance as possible. `sklearn` provides PCA in the module `sklearn.decomposition`. \n",
    "\n",
    "#### (5) Decompose the original data into 2 components in order to be displayable on a 2D graph. Verify PCA does what is expected. [1 mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_digit(0)\n",
    "data2d = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Plot all 2D (decomposed) points on a scatter plot [1 mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the data analysis we want to perform, we are often better off scaling raw data down to some desirable range and recentering them around a mean of 0. `sklearn` contains the necessary tools for this, or you can implement it yourselves.\n",
    "\n",
    "#### (7) Scale the original data so that they have a mean of 0 and a stdev of 1. Verify your results. [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) Decompose the scaled points to 2 components and plot the resulting 2D points. Are there notable differences with the plot before scaling? [1 mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_data2d = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (9) Cluster your original and your scaled data using Ward's hierarchical agglomerative clustering method. [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ward_clustering = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (10) Plot the clustering outcome similar to the 2D plot above, using a different colour for each predicted class. [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (11) Repeat the clustering using complete-link and average-link (instead of Ward) and plot as above. What do you observe? Which seems to be the better clustering method for the data and why? [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_clustering = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_clustering = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to measure the accuracy of our clusters using our labels. Note that as clustering is unsupervised we have no deterministic way of knowing which cluster corresponds to which real label. However we can make use of the fact that we knew the number of classes of the dataset (10).\n",
    "\n",
    "A simple way of measuring our clustering accuracy is the following. For each real label find the cluster which contains most of its digits. Assume that this is the cluster corresponding to this label and measure its accuracy (number of correct digits/size of the cluster). Last, average over all accuracy measures to produce the overall accuracy.\n",
    "\n",
    "#### (12) Implement a function to measure the overall accuracy of such a clustering procedure given a clustering outcome and a labels array. Use this function to measure the accuracy of `ward_clustering` and `comp_clustering`. Do you notice any potential problems with this approach? [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clustering_acc(clustering, labels):\n",
    "    \"\"\"Measure the overall accuracy of the given `clustering` given the `labels`\"\"\"\n",
    "    return 0.\n",
    "\n",
    "acc_ward = clustering_acc(ward_clustering, small_target)\n",
    "acc_comp = clustering_acc(comp_clustering, small_target)\n",
    "acc_avg = clustering_acc(avg_clustering, small_target)\n",
    "print 'Ward\\'s method:', acc_ward\n",
    "print 'Complete link:', acc_comp\n",
    "print 'Average link: ', acc_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for reducing features before clustering\n",
    "\n",
    "So far we've used PCA for displaying our data on a 2D plot. PCA can be also used to reduce the dimensionality of our samples. This results in a smaller and often more robust representation of our original data.\n",
    "\n",
    "#### (13) Reduce `small_data` to 10 features and recalculate the accuracies for Ward's method, complete- and average-link. How do they differ in comparison to using the complete feature set? (There is no need to plot anything for this step.) [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_data9d = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ward_clustering = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_clustering = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_clustering = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_ward_10d = None\n",
    "acc_comp_10d = None\n",
    "acc_avg_10d = None\n",
    "print 'Ward\\'s method:', acc_ward_10d\n",
    "print 'Complete link:', acc_comp_10d\n",
    "print 'Average link: ', acc_avg_10d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (14) Write a new function combining feature reduction and hierarchical clustering, returning an accuracy figure based on `clustering_acc()` you have created before and the clustering produced. Use the function signature below. [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_and_hcluster(n_comp, \n",
    "                        data=scaled_data, \n",
    "                        labels=small_target, \n",
    "                        linkage='ward', \n",
    "                        n_clusters=10):\n",
    "    \"\"\"Reduce and cluster the given data using hierarchical clustering\n",
    "    Arguments:\n",
    "        n_comp: the number of components to reduce the data to\n",
    "        data: the data matrix\n",
    "        labels: the correct labels, one for each data sample\n",
    "        linkage: the clustering method to use. It can be 'ward', \n",
    "                 'complete' or 'average'\n",
    "        n_clusters: the number of clusters to take into account\n",
    "    Return:\n",
    "        (The accuracy of the clustering with respect to the target labels, \n",
    "        the clustering produced)\n",
    "    \"\"\"\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (15) Using `reduce_and_hcluster()`, write a loop to calculate the accuracies for all three different linkages (ward, complete and average), for reduction of n_comp=4 to 80 with a step of 2 (i.e. for n_comp=4, 6, 8, ..., 80). Store the results in three separate lists, as indicated below. Also store the best (most accurate) clustering for each linkage. [2 marks] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_ward = best_average = best_complete = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (16) Plot the accuracies for the different linkages and number of components on a 2D plot. The x-axis should increase the number of components, while the y-axis should display accuracy. For each linkage choice also plot its accuracy before reducing the dimensions, which was calculated in step 12 - these should be straight lines. The plot should include a legend, axis titles and a grid, and each line should use a different line style. Is there anything worth observing from this plot? [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the contents of the best clustering solutions we've created in step (15).\n",
    "\n",
    "#### (17) Complete the function below. It needs to be able to display up to 100 digits from the given cluster number in the given clustering result, using `matplotlib`. If the available digits in the cluster are more than 100, choose and display 100 at random. [4 marks] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_100(clustering=best_ward, cl_no=0, data=small_data):\n",
    "    \"\"\"Display up to 100 digits from `data`, contained in the clustering product \n",
    "       `clustering` and in its cluster with number `cl_no`\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (18) Store the best clustering objects (`best_ward`, `best_complete`, `best_average`) in a single file using standard `numpy` functionality. The name of the file must be `best_clusters.npz`. Load them back and verify they are correct. [2 marks] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
